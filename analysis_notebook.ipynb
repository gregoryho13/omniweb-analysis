{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98faf102-4dbb-4260-b953-ed37ef6e1308",
   "metadata": {},
   "source": [
    "# Standalone Notebook Analysis of Magnetospheric State Query System (MSQS) data\n",
    "## Objective:\n",
    "To generate an appropriate prediction model capable of using solar wind driver parameters to predict magnetospheric state response parameters  \n",
    "This model uses the linear correlation assumption to generate linear coefficients ($\\overline {b}$) and intercepts (${a}$) for the formulas:  \n",
    "$Kp_{ijkn} = a_0 + b_0 * x_{Psw} + b_1 * x_{Bmag} + b_2 * x_{Bz} + b_3 * x_{Vsw}$  \n",
    "$AE_{ijknp} = a_0 + b_0 * x_{Psw} + b_1 * x_{Bmag} + b_2 * x_{Bz} + b_3 * x_{Vsw} + b_4 * x_{Kp}$  \n",
    "$Dst_{ijknpq} = a_0 + b_0 * x_{Psw} + b_1 * x_{Bmag} + b_2 * x_{Bz} + b_3 * x_{Vsw} + b_4 * x_{Kp} + b_5 * x_{AE}$\n",
    "### Specified parameters:  \n",
    "Driver parameters:\n",
    "* Psw - Solar wind pressure\n",
    "* Bmag - Magnitude of Interplanetary Magnetic Field (IMF) vector\n",
    "* Bz - North-South component of IMF\n",
    "* Vsw - Solar wind speed\n",
    "\n",
    "Driver & response parameters:\n",
    "* Kp - Planetary Geomagnetic Activity Index\n",
    "* Ae - Auroral Electrojet Index\n",
    "\n",
    "Response parameters:\n",
    "* Dst - Storm-time disturbance index\n",
    "\n",
    "Regression Models:\n",
    "- **Multiple Linear Regression**\n",
    "- ~~Generalized Least Squares (builds covariance matrix of size (# observations)^2; only for small samples)~~\n",
    "- ~~Ordinary Least Squares~~\n",
    "- ~~Ridge Regression (for highly correlated independent variables)~~\n",
    "- ~~Lasso Regression (shrinks coefficient values to near zero)~~\n",
    "- ~~Quantile Regression (when linear regression requirements not met or outliers in data)~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27c8cd-1d01-4f0b-807d-350ecb9187d2",
   "metadata": {},
   "source": [
    "## Changelog:  \n",
    "6/14/22:\n",
    "- Create notebook\n",
    "- New section: retrieving data (publicly)\n",
    "- New section: importing into dataframe\n",
    "\n",
    "6/15/22:\n",
    "- Edit create database: filter on relevant fields, replace fill values with null values, rename fields appropriately\n",
    "\n",
    "6/21/22:\n",
    "- Fix code to new revised objective: incorporate binning of parameters\n",
    "- New section: time shifting\n",
    "- Revised section: looping through each bin while freeing 1 parameter\n",
    "\n",
    "6/22/22:\n",
    "- Continue work on parameter shuffling component\n",
    "- Added progress indicator\n",
    "\n",
    "6/23/22:\n",
    "- Continue work on parameter shuffling\n",
    "- Added component to build linear regression models for parameter settings which produce data\n",
    "- Bug fixing\n",
    "- Freeze all parameters for each iteration except the chosen free floating variable and Dst\n",
    "\n",
    "6/25/22:\n",
    "- Reduce dataset size for testing by random sampling\n",
    "- Reduced scope of machine learning by only learning on 2-3 variables instead of all 6\n",
    "- Continue testing on finding coefficients...\n",
    "\n",
    "6/27/22:\n",
    "- Continue testing on finding coefficients by running possible coefficients through dataframe\n",
    "- Assign additional columns regarding model size (and its transformations) to learning models (as weights to coefficients)\n",
    "- High variability in frequency plot for coefficients and intercepts (several hundred?)\n",
    "\n",
    "6/28/22:\n",
    "- Write descriptions and explanations for machine learning code (Pre-processing and learning)\n",
    "\n",
    "6/29/22:\n",
    "- Add additional log transformation function to model size\n",
    "- Comparing variability of different transformation functions applied to model size (plot) (Kp has the most variability)\n",
    "- Outputs for parameter coefficients each for the different transformation functions with all using 100% of provided data\n",
    "\n",
    "7/2/22:\n",
    "- Considering different ways to split data\n",
    "- Using random sampling on dataset results in incredibly varied outputs (?)\n",
    "- More experimentation needed\n",
    "\n",
    "7/3/22:\n",
    "- Calculate prediction Dst values given coefficients; highly variable errors\n",
    "- Tweak model size weight functions to reduce error using minimizer function\n",
    "- Plot predicted Dst values against actual Dst values\n",
    "\n",
    "7/5/22:\n",
    "- Investigate cause for inaccuracies\n",
    "- Debugging main source code\n",
    "\n",
    "7/6/22:\n",
    "- Tweaked documentation\n",
    "- More debugging\n",
    "\n",
    "7/7/22:\n",
    "- Sample various time indices to compare measured and predicted Dst\n",
    "- Residual plot between actual and predicted Dst is not evenly distributed, may be due to outlier skew(?)\n",
    "\n",
    "7/8/22:\n",
    "- Tweaking with different sample sizes\n",
    "- Comparing different sample size coefficients\n",
    "\n",
    "7/11/22:\n",
    "- Save bin interval combinations for each coefficient\n",
    "- Code clean-up (Push hard-coded adjustable values to top)\n",
    "\n",
    "7/12/22:\n",
    "- Included code to extract coefficients given a configuration of bin parameters\n",
    "- Change model to pull first 100,000 entries as training data instead of random sampling\n",
    "\n",
    "7/13/22:\n",
    "- Continuing to work on transforming data to read indices of bin parameters and be able to calculate coefficients for all valid bin parameters\n",
    "\n",
    "7/14/22:\n",
    "- Issue: Manual update of finding coefficients and determining Dst estimate\n",
    "- Work on updating for all possible combinations\n",
    "\n",
    "7/15/22:\n",
    "- Continued work on updating possible combinations\n",
    "- Add tracking for valid combinations with data\n",
    "- Testing/debugging\n",
    "\n",
    "8/3/22:\n",
    "- Code bug fixing, clean-up, and more small tweaks everywhere\n",
    "- Continuous review of code logic\n",
    "\n",
    "8/4/22:\n",
    "- Logic fixing, add comments\n",
    "- More testing/debugging\n",
    "\n",
    "8/5/22:\n",
    "- Testing performance and accuracy\n",
    "- Added conventional, built-in multiple linear regression modeler from sklearn\n",
    "\n",
    "8/8/22:\n",
    "- Nothing (Prepare presentation)\n",
    "\n",
    "8/11/22:\n",
    "- Build database to store coefficients for every bin combination\n",
    "- Make Dst predictions on test data from the coefficient database\n",
    "- Will need to condense/simplify code and add comments/explanations\n",
    "- Need to run testing for data during storm events (extreme low Dst values)\n",
    "\n",
    "8/12/22:\n",
    "- Run testing for specific time intervals and plot graphs of predictions and measured values over time\n",
    "\n",
    "8/15/22:\n",
    "- Need to convert model to be able to predict Kp and AE values as well\n",
    "- Work on making model flexible enough to use appropriate driver and response parameters\n",
    "\n",
    "8/16/22:\n",
    "- Continue adjusting various parts designed for only Dst as response parameter to utilize the match variable to change its function accordingly\n",
    "\n",
    "8/17/22:\n",
    "- Finish adjusting model and run testing for all of Kp, AE, and Dst\n",
    "\n",
    "8/18/22:\n",
    "- Still need to write comments/documentation\n",
    "- Need to allow easy flexibility in changing the data range (indices) for training and testing\n",
    "- Code clean-up\n",
    "\n",
    "8/19/22:\n",
    "- Small tweaks\n",
    "- Code clean-up\n",
    "\n",
    "8/22/22:\n",
    "- Code clean-up\n",
    "- Comments\n",
    "\n",
    "8/23/22:\n",
    "- Comments\n",
    "- Small tweaks\n",
    "\n",
    "8/24/22:\n",
    "- Markdown documentation revision\n",
    "- Small tweaks\n",
    "\n",
    "8/25/22:\n",
    "- Added more easily configurable datetime plotter\n",
    "- Comments\n",
    "- Markdown documentation revision\n",
    "- Copy notebook into separate .py file\n",
    "- Add additional tester.ipynb to import .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20040997-0e96-44be-9dea-37d536b02c2b",
   "metadata": {},
   "source": [
    "## Package Imports\n",
    "* Data manipulation  \n",
    "* Machine Learning Modeler  \n",
    "* Data visualization (graphs/plots)  \n",
    "* Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90226be1-01fa-4d71-bd4a-87ef7dac1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np # For calculations and use of numpy arrays for better efficiency\n",
    "import pandas as pd # For store and organize data as pandas databases\n",
    "# Machine learning\n",
    "from sklearn.linear_model import LinearRegression # Main machine learning model to determine linear coefficients\n",
    "from sklearn.metrics import mean_squared_error # To determine the mean squared error between prediction values and actual values\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt # For plotting data\n",
    "import seaborn as sns # Alternative way of plotting data\n",
    "\n",
    "# Utilities\n",
    "from datetime import datetime, time # For converting individual dates, times, and hours to an easily (pandas/matplotlib) readable datetime format\n",
    "#from itertools import pairwise,product # Allows forming interval pairs given threshold boundaries and creating an iterable for every bin combination\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Package options (disable warnings)\n",
    "pd.options.mode.chained_assignment = None # Default to \"warn\"; To disable the warning caused by chained index calls on pandas series/databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b018dbb-753c-4ba4-953c-85bac550bbc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241m.\u001b[39mitertools\u001b[38;5;241m.\u001b[39mpairwise\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "itertools.pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40bbc8f-a6df-4f9c-a8c3-d3bc3adff7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package              Version\n",
      "-------------------- -----------\n",
      "anyio                3.5.0\n",
      "argon2-cffi          21.3.0\n",
      "argon2-cffi-bindings 21.2.0\n",
      "asttokens            2.0.5\n",
      "attrs                21.4.0\n",
      "Babel                2.9.1\n",
      "backcall             0.2.0\n",
      "beautifulsoup4       4.11.1\n",
      "bleach               4.1.0\n",
      "Bottleneck           1.3.4\n",
      "brotlipy             0.7.0\n",
      "certifi              2022.5.18.1\n",
      "cffi                 1.15.0\n",
      "charset-normalizer   2.0.4\n",
      "colorama             0.4.4\n",
      "cryptography         37.0.1\n",
      "cycler               0.11.0\n",
      "debugpy              1.5.1\n",
      "decorator            5.1.1\n",
      "defusedxml           0.7.1\n",
      "entrypoints          0.4\n",
      "executing            0.8.3\n",
      "fastjsonschema       2.15.1\n",
      "fonttools            4.25.0\n",
      "idna                 3.3\n",
      "ipykernel            6.9.1\n",
      "ipython              8.3.0\n",
      "ipython-genutils     0.2.0\n",
      "jedi                 0.18.1\n",
      "Jinja2               3.0.3\n",
      "joblib               1.1.0\n",
      "json5                0.9.6\n",
      "jsonschema           4.4.0\n",
      "jupyter-client       7.2.2\n",
      "jupyter-core         4.10.0\n",
      "jupyter-server       1.17.1\n",
      "jupyterlab           3.3.2\n",
      "jupyterlab-pygments  0.1.2\n",
      "jupyterlab-server    2.12.0\n",
      "kiwisolver           1.4.2\n",
      "MarkupSafe           2.1.1\n",
      "matplotlib           3.5.1\n",
      "matplotlib-inline    0.1.2\n",
      "mistune              0.8.4\n",
      "mkl-fft              1.3.1\n",
      "mkl-random           1.2.2\n",
      "mkl-service          2.4.0\n",
      "munkres              1.1.4\n",
      "nbclassic            0.3.5\n",
      "nbclient             0.5.13\n",
      "nbconvert            6.4.4\n",
      "nbformat             5.3.0\n",
      "nest-asyncio         1.5.5\n",
      "notebook             6.4.11\n",
      "numexpr              2.8.1\n",
      "numpy                1.22.3\n",
      "packaging            21.3\n",
      "pandas               1.4.2\n",
      "pandocfilters        1.5.0\n",
      "parso                0.8.3\n",
      "pickleshare          0.7.5\n",
      "Pillow               9.0.1\n",
      "pip                  21.2.4\n",
      "prometheus-client    0.13.1\n",
      "prompt-toolkit       3.0.20\n",
      "pure-eval            0.2.2\n",
      "pycparser            2.21\n",
      "Pygments             2.11.2\n",
      "pyOpenSSL            22.0.0\n",
      "pyparsing            3.0.4\n",
      "pyrsistent           0.18.0\n",
      "PySocks              1.7.1\n",
      "python-dateutil      2.8.2\n",
      "pytz                 2022.1\n",
      "pywin32              302\n",
      "pywinpty             2.0.2\n",
      "pyzmq                22.3.0\n",
      "requests             2.27.1\n",
      "scikit-learn         1.0.2\n",
      "scipy                1.7.3\n",
      "seaborn              0.11.2\n",
      "Send2Trash           1.8.0\n",
      "setuptools           61.2.0\n",
      "sip                  4.19.13\n",
      "six                  1.16.0\n",
      "sniffio              1.2.0\n",
      "soupsieve            2.3.1\n",
      "stack-data           0.2.0\n",
      "terminado            0.13.1\n",
      "testpath             0.5.0\n",
      "threadpoolctl        2.2.0\n",
      "tornado              6.1\n",
      "traitlets            5.1.1\n",
      "typing_extensions    4.1.1\n",
      "urllib3              1.26.9\n",
      "wcwidth              0.2.5\n",
      "webencodings         0.5.1\n",
      "websocket-client     0.58.0\n",
      "wheel                0.37.1\n",
      "win-inet-pton        1.1.0\n",
      "wincertstore         0.2\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dca98b-4eaf-4b28-bc80-8abdb6663537",
   "metadata": {},
   "source": [
    "## Model Configuration Constants\n",
    "* MATCH\n",
    "* CHANGE_MODEL_SIZE_THRESHOLD\n",
    "* matplotlib graph label sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943c8776-a0df-496e-8e37-572d9d873c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response variable to match from driver variables\n",
    "MATCH = 'Kp (n/a)' # from ['Kp (n/a)','AE (hourly) (nT)','Dst (nT)']\n",
    "\n",
    "# Above the threshold for the sample size, the machine learning linear regression estimation is used\n",
    "# Otherwise, the arithmetic mean estimation is used\n",
    "CHANGE_MODEL_SIZE_THRESHOLD = 80\n",
    "\n",
    "# Matplotlib graph label size configurations\n",
    "plt.rc('font', size=100) # controls default text sizes\n",
    "plt.rc('axes', titlesize=24) # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=20) # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=20) # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=20) # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=16) # legend fontsize\n",
    "plt.rc('figure', titlesize=18)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7058c3-841a-44f8-8bca-d2b4d0e136ab",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "* Configure boundary thresholds for each variable\n",
    "* Form bin intervals as pairs (tuples) for each variable  \n",
    "Binning Intervals:  \n",
    "$P_{SW} = [\\leq0.5;\\gt0.5-2.5;\\gt2.5-4.5;\\gt4.5-6.5;\\gt6.6-8.5;\\gt8.5-15;\\gt15]$  \n",
    "$Bmag_{SW} = [\\leq3;\\gt3-6;\\gt6-9;\\gt9-20;\\gt20]$  \n",
    "$Bz_{k_{SW}} = [\\leq-15;\\gt-15--5;\\gt-5-0;\\gt0-5;\\gt5-15;\\gt15]$  \n",
    "$V_{SW} = [\\leq300;\\gt300-400;\\gt400-500;\\gt500-600;\\gt600-700;\\gt700]$  \n",
    "$AE = [0-100;\\gt100-300;\\gt300-600;\\gt600-900;\\gt900-1200;\\gt1200]$  \n",
    "$K_{p} = [0-0.5;\\gt0.5-1.5;\\gt1.5-2.5;\\gt2.5-3.5;\\gt3.5-4.5;\\gt4.5-5.5;\\gt5.5-6.5;\\gt6.5]$  \n",
    "$Dst = [\\leq-200;\\gt-200--100;\\gt-100--50;\\gt-50--30;\\gt-30-0;\\gt0-20;\\gt20]$\n",
    "* Dictionaries to match each interval to its index for each variable\n",
    "* Lists of names for all parameters, input parameters, and output parameters\n",
    "* Dictionary of appropriate time shifts for each response parameter\n",
    "* Set matplotlib graph label size configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba665d4-3dea-4fc7-89de-09786f3e9afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'itertools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m boundaries \u001b[38;5;241m=\u001b[39m [p_bins,bmag_bins,bz_bins,v_bins,kp_bins,ae_bins,dst_bins] \u001b[38;5;66;03m# List of bin boundaries for each variable\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# *Note: AE and Kp intervals use -0.001 instead of 0 to include 0 within the interval\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Bin intervals (Ex. [(-inf,3),(3,6),...,(20,inf)] bmag)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m bins \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mpairwise(b)) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m boundaries] \u001b[38;5;66;03m# List of bin intervals for each variable\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Array of dictionaries matching the interval to its index (Ex. (-inf,3) -> 0; (3,6) -> 1; ... ; (20,inf) -> 4) (for bmag)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m indices \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m boundaries \u001b[38;5;241m=\u001b[39m [p_bins,bmag_bins,bz_bins,v_bins,kp_bins,ae_bins,dst_bins] \u001b[38;5;66;03m# List of bin boundaries for each variable\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# *Note: AE and Kp intervals use -0.001 instead of 0 to include 0 within the interval\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Bin intervals (Ex. [(-inf,3),(3,6),...,(20,inf)] bmag)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m bins \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[43mitertools\u001b[49m\u001b[38;5;241m.\u001b[39mpairwise(b)) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m boundaries] \u001b[38;5;66;03m# List of bin intervals for each variable\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Array of dictionaries matching the interval to its index (Ex. (-inf,3) -> 0; (3,6) -> 1; ... ; (20,inf) -> 4) (for bmag)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m indices \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'itertools' is not defined"
     ]
    }
   ],
   "source": [
    "# Bin boundaries/thresholds (Ex. [-inf, 3, 6,...,inf] bmag)\n",
    "p_bins = [-np.inf,0.5,2.5,4.5,6.5,8.5,15,np.inf] # 7 intervals\n",
    "bmag_bins = [-np.inf,3,6,9,20,np.inf] # 5 intervals\n",
    "bz_bins = [-np.inf,-15,-5,0,5,15,np.inf] # 6 intervals\n",
    "v_bins = [-np.inf,300,400,500,600,700,np.inf] # 6 intervals\n",
    "ae_bins = [-0.001,100,300,600,900,1200,np.inf] # 6 intervals*\n",
    "kp_bins = [-0.001,0.5,1.5,2.5,3.5,4.5,5.5,6.5,np.inf] # 8 intervals*\n",
    "dst_bins = [-np.inf,-200,-100,-50,-30,0,20,np.inf] # 7 intervals\n",
    "\n",
    "boundaries = [p_bins,bmag_bins,bz_bins,v_bins,kp_bins,ae_bins,dst_bins] # List of bin boundaries for each variable\n",
    "# *Note: AE and Kp intervals use -0.001 instead of 0 to include 0 within the interval\n",
    "\n",
    "# Bin intervals (Ex. [(-inf,3),(3,6),...,(20,inf)] bmag)\n",
    "bins = [list(pairwise(b)) for b in boundaries] # List of bin intervals for each variable\n",
    "\n",
    "# Array of dictionaries matching the interval to its index (Ex. (-inf,3) -> 0; (3,6) -> 1; ... ; (20,inf) -> 4) (for bmag)\n",
    "indices = []\n",
    "for b in bins:\n",
    "    interval_indices = {}\n",
    "    for idx, interval in enumerate(b):\n",
    "        interval_indices[interval] = idx\n",
    "    interval_indices[(-np.inf, np.inf)] = -1\n",
    "    indices.append(interval_indices)\n",
    "    \n",
    "# Lists of names of parameters\n",
    "parameters = ['SW Flow Pressure (nPa)','SW Bmag (nT)','SW Bz (nT)','SW Velocity (km/sec)','Kp (n/a)','AE (hourly) (nT)','Dst (nT)'] # Name of parameters in order of provided bins\n",
    "io_index = parameters.index(MATCH) # The index of the response parameter in the list of all parameters\n",
    "input_parameters = parameters[:io_index] # List of all input parameters excluding the response parameter\n",
    "output_parameters = parameters[io_index] # The output parameter (should just be one)\n",
    "\n",
    "# How much to time shift the driver parameters to match the corresponding response parameters (by hours)\n",
    "time_shifts = {\n",
    "    'Kp (n/a)': {\n",
    "        'SW Flow Pressure (nPa)': 0, # Default 0 hours\n",
    "        'SW Bmag (nT)': 0, # 0 hours\n",
    "        'SW Bz (nT)': 1, # 1 hour\n",
    "        'SW Velocity (km/sec)': 0, # 0 hours\n",
    "        'Kp (n/a)': 0, # 0 hours\n",
    "        'AE (hourly) (nT)': 0 # 0 hours\n",
    "    },\n",
    "    'AE (hourly) (nT)': {\n",
    "        'SW Flow Pressure (nPa)': 0, # Default 0 hours\n",
    "        'SW Bmag (nT)': 0, # 0 hours\n",
    "        'SW Bz (nT)': 1, # 1 hour\n",
    "        'SW Velocity (km/sec)': 0, # 0 hours\n",
    "        'Kp (n/a)': 0, # 0 hours\n",
    "        'AE (hourly) (nT)': 0 # 0 hours\n",
    "    },\n",
    "    'Dst (nT)': {\n",
    "        'SW Flow Pressure (nPa)': 2, # Default 2 hours\n",
    "        'SW Bmag (nT)': 2, # 2 hours\n",
    "        'SW Bz (nT)': 3, # 3 hours\n",
    "        'SW Velocity (km/sec)': 2, # 2 hours\n",
    "        'Kp (n/a)': 2, # 2 hours\n",
    "        'AE (hourly) (nT)': 2 # 2 hours\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cebef2-1793-4146-8fa2-c8a7fe57bc91",
   "metadata": {},
   "source": [
    "### Retrieving available data (if data does not already exist)\n",
    "To retrieve public MSQS data:\n",
    "1. Pull public MSQS data from https://omniweb.gsfc.nasa.gov/ow.html as .lst and .fmt files\n",
    "2. Convert files into formatted .csv file (Excel) (.lst -> .txt -> .xlsx -> .csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4651f-6126-4e45-be13-26b9163a6757",
   "metadata": {},
   "source": [
    "### Create database\n",
    "Read csv file into pandas dataframe  \n",
    "Replace fill values with NaN (Not a Number) values  \n",
    "Specific fill values for each category can be referenced from https://omniweb.gsfc.nasa.gov/html/ow_data.html#3  \n",
    "Included data:\n",
    "* Driver parameters:\n",
    "    * Psw - Solar wind pressure\n",
    "    * Bmag - Magnitude of Interplanetary Magnetic Field (IMF) vector\n",
    "    * Bz - North-South component of IMF\n",
    "    * Vsw - Solar wind speed\n",
    "* Driver & response parameters:\n",
    "    * Kp - Planetary Geomagnetic Activity Index\n",
    "    * Ae - Auroral Electrojet Index\n",
    "* Response parameters:\n",
    "    * Dst - Storm-time disturbance index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27500bcc-da9a-4535-8862-38f4b1c99d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local csv file (must be in same directory as this file)\n",
    "raw_df = pd.read_csv('omni2_h8ZIWOAzck.csv')\n",
    "\n",
    "# Relevant fields in data pulling from the raw data\n",
    "fields = [\n",
    "    'Year',\n",
    "    'DOY',\n",
    "    'Hour',\n",
    "    'Flow pressure',\n",
    "    'Vector B Magnitude,nT',\n",
    "    # 'BX, nT (GSE, GSM)',\n",
    "    # 'BY, nT (GSM)',\n",
    "    'BZ, nT (GSM)',\n",
    "    'SW Plasma Speed, km/s',\n",
    "    'Kp index',\n",
    "    'AE-index, nT',\n",
    "    'Dst-index, nT',\n",
    "    # 'R (Sunspot No.)',\n",
    "    # 'f10.7_index',\n",
    "    # 'SW Plasma Temperature, K',\n",
    "    # 'SW Proton Density, N/cm^3',\n",
    "    # 'E elecrtric field', # Is misspelled in .csv file\n",
    "    # 'Plasma Beta',\n",
    "    # 'Quasy-Invariant',\n",
    "    # 'Alfen mach number',\n",
    "    # 'AL-index, nT',\n",
    "    # 'AU-index, nT',\n",
    "    # 'ap_index, nT',\n",
    "    # 'pc-index'\n",
    "]\n",
    "\n",
    "# Fill values for each category where the data is null or missing (these values in the categories represent missing values and should be replaced by null)\n",
    "fill_values = {\n",
    "    'Flow pressure': 99.99,\n",
    "    'Vector B Magnitude,nT': 999.9,\n",
    "    'SW Plasma Speed, km/s': 9999.,\n",
    "    'BX, nT (GSE, GSM)': 999.9,\n",
    "    'BY, nT (GSM)': 999.9,\n",
    "    'BZ, nT (GSM)': 999.9,\n",
    "    'SW Plasma Speed, km/s': 9999.,\n",
    "    'Kp index': 99,\n",
    "    'Dst-index, nT': 99999,\n",
    "    'AE-index, nT': 9999,\n",
    "    # 'SW Plasma Temperature, K': 9999999.,\n",
    "    # 'Plasma Beta': 999.99,\n",
    "    # 'Quasy-Invariant': 9.9999,\n",
    "    # 'Alfen mach number': 999.9,\n",
    "    # 'AL-index, nT': 99999,\n",
    "    # 'AU-index, nT': 99999,\n",
    "    # 'ap_index, nT': 999,\n",
    "    # 'pc-index': 999.9\n",
    "}\n",
    "\n",
    "# Renames for each category\n",
    "renames = {\n",
    "    'DOY':'Decimal Day',\n",
    "    'Hour':'hour:min',\n",
    "    'Flow pressure': 'SW Flow Pressure (nPa)',\n",
    "    'Vector B Magnitude,nT': 'SW Bmag (nT)',\n",
    "    'BX, nT (GSE, GSM)': 'SW Bx (nT)',\n",
    "    'BY, nT (GSM)': 'SW By (nT)',\n",
    "    'BZ, nT (GSM)': 'SW Bz (nT)',\n",
    "    'SW Plasma Speed, km/s': 'SW Velocity (km/sec)',\n",
    "    'Kp index': 'Kp (n/a)',\n",
    "    'Dst-index, nT': 'Dst (nT)',\n",
    "    'AE-index, nT': 'AE (hourly) (nT)',\n",
    "    # 'R (Sunspot No.)': 'Sunspot Number R (n/a)',\n",
    "    # 'f10.7_index': 'F10.7 (10^-22 Joul/sec/m^2/Hz)',\n",
    "    # 'SW Plasma Temperature, K': 'SW Proton Temperature (K)', # May not match\n",
    "    # 'SW Proton Density, N/cm^3': 'SW Proton Density (/cc)',\n",
    "    # 'E elecrtric field': 'SW Electric Field (mV/m)',\n",
    "    # 'Plasma Beta': 'Plasma Beta (n/a)',\n",
    "    # 'Quasy-Invariant': 'Quasi-Invariant (n/a)',\n",
    "    # 'Alfen mach number': 'Alfven Mach Number (n/a)',\n",
    "    # 'AL-index, nT': 'AL (hourly) (nT)',\n",
    "    # 'AU-index, nT': 'AU (hourly) (nT)',\n",
    "    # 'ap_index, nT': 'AP Index (nT)',\n",
    "    # 'pc-index': 'PC(N) Index (n/a)'\n",
    "}\n",
    "\n",
    "# Replace the specific fill values with null for each category appropriately, and rename the categories appropriately\n",
    "raw_df = raw_df.replace(fill_values, np.nan)[fields].rename(columns=renames)\n",
    "\n",
    "# Kp data from imported data is multiplied by 10 to be in integer format\n",
    "# Readjust Kp data by dividing by 10 and converting to decimal/float\n",
    "if(raw_df['Kp (n/a)'].dtypes == 'int64'):\n",
    "    raw_df['Kp (n/a)'] = raw_df['Kp (n/a)']/10.\n",
    "\n",
    "df = raw_df # Copy dataframe into another dataframe before time shifting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d275d-6703-44b6-b095-72c56112cd91",
   "metadata": {},
   "source": [
    "### Database set-up cont.\n",
    "Time shift each column appropriately:  \n",
    "For Kp & AE:  \n",
    "* $Bz_{t-1} \\sim Kp_{t} \\sim AE_{t}$\n",
    "\n",
    "For Dst:\n",
    "* $V_{SW_{t-2}} \\sim B_{mag_{t-2}} \\sim P_{SW_{t-2}} \\sim Dst_{t}$\n",
    "* $Kp_{t-2} \\sim AE_{t-2} \\sim Dst_{t}$\n",
    "* $Bz_{t-3} \\sim Dst_{t}$\n",
    "\n",
    "\n",
    "Drop all entries with missing values  \n",
    "Combine time columns to one easily readable (by pandas and matplotlib) format  \n",
    "Split training and testing data from modified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f52919-a2fa-4f6c-b126-534c743d99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve columns from SW Flow Pressure to Dst and time shift each column appropriately\n",
    "for col_name in df.columns[3:9]:\n",
    "    df[col_name] = df[col_name].shift(time_shifts[MATCH][col_name])\n",
    "\n",
    "df = df.dropna() # Drop all entries where there are any missing values in any of the columns\n",
    "# dataframe df should have 288249 rows after\n",
    "\n",
    "# Combine date and time columns to one datetime column (Ex. year:1999, day:25, hour:22 => 199902506 => 1999-06-25 22:00:00)\n",
    "df['datetime'] = pd.to_datetime(df['Year'] * 100000 + df['Decimal Day'] * 100 + df['hour:min'], format='%Y%j%H') # YYYYDDDHH\n",
    "\n",
    "# Use first 100,000 entries as training data and rest as testing data (arbitrary)\n",
    "train = df[:100000]\n",
    "test = df[100000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d3ed0-aa76-4e43-9459-2c7de533900d",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "Multi-parameter Linear Equations:  \n",
    "$Kp_{ijkn} = a_0 + b_0 * x_{Psw} + b_1 * x_{Bmag} + b_2 * x_{Bz} + b_3 * x_{Vsw}$  \n",
    "$AE_{ijknp} = a_0 + b_0 * x_{Psw} + b_1 * x_{Bmag} + b_2 * x_{Bz} + b_3 * x_{Vsw} + b_4 * x_{Kp}$  \n",
    "$Dst_{ijknpq} = a_0 + b_0 * x_{Psw} + b_1 * x_{Bmag} + b_2 * x_{Bz} + b_3 * x_{Vsw} + b_4 * x_{Kp} + b_5 * x_{AE}$  \n",
    "\n",
    "Assumptions of Regression Model:\n",
    "1. ~~Linearity (Assumption made in building model)~~\n",
    "2. Homoscedasticity\n",
    "3. Independence\n",
    "4. Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4e6cb-4b58-43e0-aac6-60db561b899f",
   "metadata": {},
   "source": [
    "## Main Machine learning code\n",
    "### Pre-processing:  \n",
    "- Initialize the response parameter estimate column\n",
    "- Set bin indices for response parameter and unused parameters to (-inf, inf)\n",
    "- Initialize storage for coefficient sets of each bin combination\n",
    "\n",
    "### Learning:\n",
    "- Binning combinations:\n",
    "    - Data is split for each possible combination of the variables (each iteration, data is stored in loc)\n",
    "    - Not all combinations result in data output, only combinations with at least one data point will be processed through learning model\n",
    "    - (One known combination: p:(-infinity,0.5), bmag:(3,6), bz:(0,5), v:(300,400), ae:(0,100), kp:(0.5,1.5), dst:(0,20); 0 1 3 1 0 1 5 )\n",
    "- Model building:\n",
    "    - Pull data within all the bin parameters\n",
    "    - Only the response variables and unused variables should have their bin parameters set to (-infinity, infinity)\n",
    "    - Build linear regression model with (all of) the filtered data by fitting the input parameters with the output parameters\n",
    "        - The model determines the linear coefficients for each input parameter to the output parameter including the intercept simultaneously\n",
    "    - Return the linear regression coefficients, intercept, and model size for each successful model built\n",
    "- Structure:\n",
    "    - Iterate through every possible bin combination for all of the variables\n",
    "        - If there is data from a certain bin combination, build a linear model and store its linear coefficients in coeff_df\n",
    "        - Also updates the result dataframe with the response parameter estimate given the linear equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e619b64-68ae-4c3f-a1e9-75bf840473e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Copy training data to result data and add response parameter estimate\n",
    "result = train\n",
    "result[MATCH + ' est'] = np.nan\n",
    "\n",
    "# Appropriately set bin indices for the response parameter to -inf, inf\n",
    "if MATCH == 'Dst (nT)':\n",
    "    boundaries[6] = [-np.inf,np.inf] # Dst boundaries\n",
    "elif MATCH == 'AE (hourly) (nT)':\n",
    "    boundaries[6] = [-np.inf,np.inf] # Dst boundaries\n",
    "    boundaries[5] = [-np.inf,np.inf] # AE boundaries\n",
    "elif MATCH == 'Kp (n/a)':\n",
    "    boundaries[6] = [-np.inf,np.inf] # Dst boundaries\n",
    "    boundaries[5] = [-np.inf,np.inf] # AE boundaries\n",
    "    boundaries[4] = [-np.inf,np.inf] # Kp boundaries\n",
    "\n",
    "# Counter for number of coefficient sets stored, initialize coefficient sets size to 9000 (arbitrary)\n",
    "coeff_iter = 0\n",
    "coeffs = np.empty((9000, 10 + len(input_parameters)))\n",
    "\n",
    "# Form an iterable object cycling through all the possible bin combination intervals\n",
    "# Form list of intervals between each boundary for each variable\n",
    "# Make complete list of all combination of intervals for each variable\n",
    "# Iterate through each combination\n",
    "product_pairs = product(pairwise(boundaries[0]),pairwise(boundaries[1]),pairwise(boundaries[2]),pairwise(boundaries[3]),pairwise(boundaries[4]),pairwise(boundaries[5]),pairwise(boundaries[6]))\n",
    "for p_interval,bmag_interval,bz_interval,v_interval,kp_interval,ae_interval,dst_interval in product_pairs:\n",
    "    # clear_output(wait=True)\n",
    "    \n",
    "    # Retrieve the data for a bin combination and store as loc\n",
    "    # Get data with Dst and the current free variable as free and the other variables restricted to bins\n",
    "    # Each of the parameter limits are stored in p_interval, bmag_interval,...,dst_interval\n",
    "    # If a parameter is set to be free, then the parameter interval is set to (-infinity,infinity)\n",
    "\n",
    "    # The below statement returns data within the boundaries of a bin parameter combination...\n",
    "    # which is then run for each possible combination...\n",
    "    # which is repeated for setting a different parameter to be free\n",
    "    loc = train.loc[(train['SW Flow Pressure (nPa)']>p_interval[0]) & (train['SW Flow Pressure (nPa)']<=p_interval[1]) & \n",
    "            (train['SW Bmag (nT)']>bmag_interval[0]) & (train['SW Bmag (nT)']<=bmag_interval[1]) &\n",
    "            (train['SW Bz (nT)']>bz_interval[0]) & (train['SW Bz (nT)']<=bz_interval[1]) &\n",
    "            (train['SW Velocity (km/sec)']>v_interval[0]) & (train['SW Velocity (km/sec)']<=v_interval[1]) &\n",
    "            (train['Kp (n/a)']>kp_interval[0]) & (train['Kp (n/a)']<=kp_interval[1]) &\n",
    "            (train['AE (hourly) (nT)']>ae_interval[0]) & (train['AE (hourly) (nT)']<=ae_interval[1]) &\n",
    "            (train['Dst (nT)']>dst_interval[0]) & (train['Dst (nT)']<=dst_interval[1])]\n",
    "    \n",
    "    # If there is data for the bin combination, make a linear regression model to fit the input parameters with the output parameters\n",
    "    if loc.shape[0] > 0:\n",
    "        lin_regr = LinearRegression()\n",
    "        lin_regr.fit(loc[input_parameters],loc[output_parameters])\n",
    "        \n",
    "        # Calculates the estimates for the output parameter according to whichever output parameter is assigned\n",
    "        # The column of response estimates are calculated according to the linear equations given previously\n",
    "        if MATCH == 'Dst (nT)':\n",
    "            loc['Dst (nT) est'] = (loc['SW Flow Pressure (nPa)'] * lin_regr.coef_[0] \\\n",
    "                + loc['SW Bmag (nT)'] * lin_regr.coef_[1] \\\n",
    "                + loc['SW Bz (nT)'] * lin_regr.coef_[2] \\\n",
    "                + loc['SW Velocity (km/sec)'] * lin_regr.coef_[3] \\\n",
    "                + loc['Kp (n/a)'] * lin_regr.coef_[4] \\\n",
    "                + loc['AE (hourly) (nT)'] * lin_regr.coef_[5] \\\n",
    "                + lin_regr.intercept_)\n",
    "        elif MATCH == 'AE (hourly) (nT)':\n",
    "             loc['AE (hourly) (nT) est'] = (loc['SW Flow Pressure (nPa)'] * lin_regr.coef_[0] \\\n",
    "                + loc['SW Bmag (nT)'] * lin_regr.coef_[1] \\\n",
    "                + loc['SW Bz (nT)'] * lin_regr.coef_[2] \\\n",
    "                + loc['SW Velocity (km/sec)'] * lin_regr.coef_[3] \\\n",
    "                + loc['Kp (n/a)'] * lin_regr.coef_[4] \\\n",
    "                + lin_regr.intercept_)\n",
    "        else:\n",
    "            loc['Kp (n/a) est'] = (loc['SW Flow Pressure (nPa)'] * lin_regr.coef_[0] \\\n",
    "                + loc['SW Bmag (nT)'] * lin_regr.coef_[1] \\\n",
    "                + loc['SW Bz (nT)'] * lin_regr.coef_[2] \\\n",
    "                + loc['SW Velocity (km/sec)'] * lin_regr.coef_[3] \\\n",
    "                + lin_regr.intercept_)\n",
    "        \n",
    "        interval_indices = [indices[idx][interval] for idx, interval in enumerate([p_interval, bmag_interval, bz_interval, v_interval, kp_interval, ae_interval, dst_interval])]\n",
    "        \n",
    "        coeffs[coeff_iter] = interval_indices + list(lin_regr.coef_) + [lin_regr.intercept_, loc[MATCH].mean(), loc.shape[0]]\n",
    "        \n",
    "        result.update(loc, overwrite=False)\n",
    "        coeff_iter += 1\n",
    "    \n",
    "coeffs.resize((coeff_iter), 10 + len(input_parameters))\n",
    "\n",
    "# Approximate runtime: 7 min\n",
    "# Size of coeffs: 4906 sets of coefficients for Dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a05678-72fe-42af-9bbf-0f6f7f358a6f",
   "metadata": {},
   "source": [
    "### Build coefficients dataframe\n",
    "Coefficients for every bin combination  \n",
    "Each entry has a bin index for all parameters, coefficients for all input parameters, intercept, the arithmetic mean for the output parameter, and the model/sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9458846f-6ab7-4a5f-b5cd-ee94d33d347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['SW Flow Pressure (nPa)', 'SW Bmag (nT)', 'SW Bz (nT)', 'SW Velocity (km/sec)', 'Kp (n/a)', 'AE (hourly) (nT)', 'Dst (nT)'] \\\n",
    "            + [s + ' coeff' for s in input_parameters] \\\n",
    "            + ['intercept', 'arithmetic_mean', 'model_size']\n",
    "coeff_df = pd.DataFrame(coeffs, columns=columns).convert_dtypes()\n",
    "coeff_df.sort_values('model_size', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47711c-0377-4a94-9f4b-66aa72be792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of indices to list of corresponding intervals\n",
    "# Ex. [0, 0, 2, 0, 0, 0, -1] -> [(-inf, 0.5), (-inf, 3), (-5, 0), (-inf, 300), (-0.001, 0.5), (-0.001, 100), (-inf, inf)]\n",
    "def index_to_interval(indices, bins):\n",
    "    intervals = []\n",
    "    for index, b in zip(indices, bins):\n",
    "        if int(index) == -1:\n",
    "            intervals.append((-np.inf, np.inf))\n",
    "        else:\n",
    "            intervals.append(b[int(index)])\n",
    "        \n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffeed36-d9a7-47b5-a493-9f42fc13d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[MATCH + ' est'] = np.nan\n",
    "\n",
    "for idx, row in coeff_df.iterrows():\n",
    "    loc_indices = list(row[['SW Flow Pressure (nPa)','SW Bmag (nT)','SW Bz (nT)','SW Velocity (km/sec)','Kp (n/a)','AE (hourly) (nT)', 'Dst (nT)']])\n",
    "    loc_intervals = index_to_interval(loc_indices, bins)\n",
    "    \n",
    "    loc = test.loc[(test['SW Flow Pressure (nPa)']>loc_intervals[0][0]) & (test['SW Flow Pressure (nPa)']<=loc_intervals[0][1]) & \n",
    "            (test['SW Bmag (nT)']>loc_intervals[1][0]) & (test['SW Bmag (nT)']<=loc_intervals[1][1]) &\n",
    "            (test['SW Bz (nT)']>loc_intervals[2][0]) & (test['SW Bz (nT)']<=loc_intervals[2][1]) &\n",
    "            (test['SW Velocity (km/sec)']>loc_intervals[3][0]) & (test['SW Velocity (km/sec)']<=loc_intervals[3][1]) &\n",
    "            (test['Kp (n/a)']>loc_intervals[4][0]) & (test['Kp (n/a)']<=loc_intervals[4][1]) &\n",
    "            (test['AE (hourly) (nT)']>loc_intervals[5][0]) & (test['AE (hourly) (nT)']<=loc_intervals[5][1]) &\n",
    "            (test['Dst (nT)']>loc_intervals[6][0]) & (test['Dst (nT)']<=loc_intervals[6][1])]\n",
    "    if loc.shape[0] > 0:\n",
    "        if row['model_size'] > CHANGE_MODEL_SIZE_THRESHOLD:\n",
    "            coeffs = list(row[[s + ' coeff' for s in input_parameters]])\n",
    "            \n",
    "            if MATCH == 'Dst (nT)':\n",
    "                loc['Dst (nT) est'] = (loc['SW Flow Pressure (nPa)'] * coeffs[0] \\\n",
    "                    + loc['SW Bmag (nT)'] * coeffs[1] \\\n",
    "                    + loc['SW Bz (nT)'] * coeffs[2] \\\n",
    "                    + loc['SW Velocity (km/sec)'] * coeffs[3] \\\n",
    "                    + loc['Kp (n/a)'] * coeffs[4] \\\n",
    "                    + loc['AE (hourly) (nT)'] * coeffs[5] \\\n",
    "                    + row['intercept'])\n",
    "            elif MATCH == 'AE (hourly) (nT)':\n",
    "                 loc['AE (hourly) (nT) est'] = (loc['SW Flow Pressure (nPa)'] * coeffs[0] \\\n",
    "                    + loc['SW Bmag (nT)'] * coeffs[1] \\\n",
    "                    + loc['SW Bz (nT)'] * coeffs[2] \\\n",
    "                    + loc['SW Velocity (km/sec)'] * coeffs[3] \\\n",
    "                    + loc['Kp (n/a)'] * coeffs[4] \\\n",
    "                    + row['intercept'])\n",
    "            else:\n",
    "                loc['Kp (n/a) est'] = (loc['SW Flow Pressure (nPa)'] * coeffs[0] \\\n",
    "                    + loc['SW Bmag (nT)'] * coeffs[1] \\\n",
    "                    + loc['SW Bz (nT)'] * coeffs[2] \\\n",
    "                    + loc['SW Velocity (km/sec)'] * coeffs[3] \\\n",
    "                    + row['intercept'])\n",
    "\n",
    "        else:\n",
    "            loc[MATCH + ' est'] = row['arithmetic_mean']\n",
    "            \n",
    "        test.update(loc, overwrite=False)\n",
    "\n",
    "# Compute prediction errors between predicted values and actual values\n",
    "test[MATCH + ' diff'] = (test[MATCH] - test[MATCH + ' est'])\n",
    "\n",
    "# Reformat data in dataframe to most fitting data format\n",
    "test = test.convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c5ba3-93e5-47b1-a892-bd9b8b5ea4c5",
   "metadata": {},
   "source": [
    "# Results\n",
    "Resulting test dataframe with parameter estimates and differences between estimates and actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1969c7-8fe5-4789-9045-c2dfee9c2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b168d8d-5968-4685-a0cd-345251c67d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (# entries with a valid estimation for the response parameter, # entries with no estimation)\n",
    "test[test[MATCH + ' est'].notnull()].shape[0], test[test[MATCH + ' est'].isnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b066321-c132-4f7e-ae40-6861590f830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test[MATCH + ' est'].notnull()].reindex(test[test[MATCH + ' est'].notnull()][MATCH + ' diff'].abs().sort_values().index) # Sorted by increasing estimation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56f59a-6311-4bab-a003-f1c53a6ffd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error between actual response values vs predicted response values (smaller = more accurate)\n",
    "mean_squared_error(test[test[MATCH + ' est'].notnull()][MATCH].astype('float64'),test[test[MATCH + ' est'].notnull()][MATCH + ' est'].astype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcea09-5c15-45be-aa39-c3f98a2b0df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[MATCH + ' diff'].mean(), test[MATCH + ' diff'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73cb886-ca12-4295-a359-12104f3c6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(test[MATCH + ' diff'], kde=True, bins=100)\n",
    "plt.legend([MATCH + ' error'])\n",
    "#plt.xlim((-100,100))\n",
    "plt.xlabel(MATCH + ' error/difference')\n",
    "plt.title('Error distribution for '+MATCH+' across all testing data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868e2f1-f196-4c65-86e8-5379975e55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,10))\n",
    "\n",
    "# alpha to set opacity\n",
    "test.set_index('datetime')[MATCH].plot(alpha=0.7)\n",
    "test.set_index('datetime')[MATCH + ' est'].plot(alpha=0.7)\n",
    "\n",
    "#plt.ylim((-500,500))\n",
    "plt.xlabel('Time')\n",
    "plt.title(MATCH + ' prediction and actual values for all testing data from '+test['datetime'].iloc[0].strftime('%m/%d/%Y %H:%M')+' to '+test['datetime'].iloc[-1].strftime('%m/%d/%Y %H:%M'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8950208e-5f56-4427-a2d6-60e3b49db4cb",
   "metadata": {},
   "source": [
    "### Configurable datetime range graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a9c39-c18a-4aba-bcdb-4080d33c83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,10))\n",
    "\n",
    "time_interval = (datetime(2000, 4, 2, 11, 0),datetime(2000, 10, 1, 16, 0)) # Configurable datetime(year, month, day, hour, min)\n",
    "# Due to nature of Python slicing, the first datetime inclusive and second datetime exclusive\n",
    "\n",
    "test.set_index('datetime')[MATCH][time_interval[0]:time_interval[1]].plot(alpha=0.7) # alpha to set opacity\n",
    "test.set_index('datetime')[MATCH + ' est'][time_interval[0]:time_interval[1]].plot(alpha=0.7)\n",
    "\n",
    "#plt.ylim((-320,100))\n",
    "plt.xlabel('Time')\n",
    "plt.title(MATCH+' predicted and actual values from '+time_interval[0].strftime('%m/%d/%Y %H:%M')+' to '+time_interval[1].strftime('%m/%d/%Y %H:%M'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c85e1e-2cd5-4a71-b84e-1c917839662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,10))\n",
    "\n",
    "time_interval = (32400,36500) # By index\n",
    "# Due to nature of Python slicing, the first datetime inclusive and second datetime exclusive\n",
    "\n",
    "test.set_index('datetime')[MATCH][time_interval[0]:time_interval[1]].plot(alpha=0.7)\n",
    "test.set_index('datetime')[MATCH + ' est'][time_interval[0]:time_interval[1]].plot(alpha=0.7)\n",
    "\n",
    "#plt.ylim((-320,100))\n",
    "plt.xlabel('Time')\n",
    "plt.title(MATCH+' predicted and actual values from '+test['datetime'].iloc[time_interval[0]].strftime('%m/%d/%Y %H:%M')+' to '+test['datetime'].iloc[time_interval[1]].strftime('%m/%d/%Y %H:%M'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1da25-9f36-4c10-a40c-1587fdd4a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,10))\n",
    "\n",
    "time_interval = (36500,40600) # By index\n",
    "# Due to nature of Python slicing, the first datetime inclusive and second datetime exclusive\n",
    "\n",
    "test.set_index('datetime')[MATCH][time_interval[0]:time_interval[1]].plot(alpha=0.7)\n",
    "test.set_index('datetime')[MATCH + ' est'][time_interval[0]:time_interval[1]].plot(alpha=0.7)\n",
    "\n",
    "#plt.ylim((-420,100))\n",
    "plt.xlabel('Time')\n",
    "plt.title(MATCH+' predicted and actual values from '+test['datetime'].iloc[time_interval[0]].strftime('%m/%d/%Y %H:%M')+' to '+test['datetime'].iloc[time_interval[1]].strftime('%m/%d/%Y %H:%M'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29089cce-64e5-4e77-8483-aea501a9a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,10))\n",
    "\n",
    "time_interval = (36500,36800) # By index\n",
    "# Due to nature of Python slicing, the first datetime inclusive and second datetime exclusive\n",
    "\n",
    "test.set_index('datetime')[MATCH][time_interval[0]:time_interval[1]].plot()\n",
    "test.set_index('datetime')[MATCH + ' est'][time_interval[0]:time_interval[1]].plot()\n",
    "\n",
    "#plt.ylim((-420,100))\n",
    "plt.xlabel('Time')\n",
    "plt.title(MATCH+' predicted and actual values from '+test['datetime'].iloc[time_interval[0]].strftime('%m/%d/%Y %H:%M')+' to '+test['datetime'].iloc[time_interval[1]].strftime('%m/%d/%Y %H:%M'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa727bc-9001-4aaf-8599-afbb2687ff1e",
   "metadata": {},
   "source": [
    "### ~~UNUSED Weighted means computations~~\n",
    "~~Compute weighted means of coefficients using different function transformations of the sample size~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c30836-e151-4c05-ba2c-8751d7d413d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_func(x, df, coeff):\n",
    "    df['Dst est'] = (df['SW Flow Pressure (nPa)'] * coeff[0] \\\n",
    "        + df['SW Bmag (nT)'] * coeff[1] \\\n",
    "        + df['SW Bz (nT)'] * coeff[2] \\\n",
    "        + df['SW Velocity (km/sec)'] * coeff[3] \\\n",
    "        + df['Kp (n/a)'] * coeff[4] \\\n",
    "        + df['AE (hourly) (nT)'] * coeff[5]\n",
    "        + x )\n",
    "    return mean_squared_error(df['Dst (nT)'],df['Dst est'])\n",
    "\n",
    "def power_func(x):\n",
    "    power_weight_means = []\n",
    "\n",
    "    for variable_data in regression_data:\n",
    "        regr_df = pd.DataFrame(variable_data, columns=['coefficient','intercept','model_size','free_var','p_interval','bmag_interval','bz_interval','v_interval','kp_interval','ae_interval','dst_interval'])\n",
    "        regr_df['model_size_power'] = np.power(regr_df['model_size'],x)\n",
    "\n",
    "        power_weight_mean = np.sum(regr_df['coefficient']*regr_df['model_size_power']/np.sum(regr_df['model_size_power']))\n",
    "        power_weight_means.append(power_weight_mean)\n",
    "    \n",
    "    m = minimize_scalar(weight_func, args=(df, power_weight_means))\n",
    "    return m.fun\n",
    "\n",
    "# m2 = minimize_scalar(power_func)\n",
    "# mse = m2.fun\n",
    "# power = m2.x\n",
    "# m2 # fun: mean squared error; x: power yielding lowest mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b1590-f078-4e84-a0d2-540d1e882659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_data(df, i, j, k, n, p, q):\n",
    "    return df.loc[((df['SW Flow Pressure (nPa)'] == i) | (df['SW Flow Pressure (nPa)'] == -1)) &\n",
    "               ((df['SW Bmag (nT)'] == j) | (df['SW Bmag (nT)'] == -1)) &\n",
    "               ((df['SW Bz (nT)'] == k) | (df['SW Bz (nT)'] == -1)) &\n",
    "               ((df['SW Velocity (km/sec)'] == n) | (df['SW Velocity (km/sec)'] == -1)) &\n",
    "               ((df['Kp (n/a)'] == p) | (df['Kp (n/a)'] == -1)) &\n",
    "               ((df['AE (hourly) (nT)'] == q) | (df['AE (hourly) (nT)'] == -1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28f1b9-7183-49b0-ae0b-a5289be451a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data(coeff_df, 1, 0, 2, 3, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11a428-370a-4adb-ae47-7fcb6fc4c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for config in valid_combinations:\n",
    "#     i,j,k,n,p,q,_ = config\n",
    "    \n",
    "#     # Data on models with the set configuration where each row has a different free variable set\n",
    "#     # Should return 5 rows for 5 different free variables (for Dst)\n",
    "#     loc = config_data(regr_df, i, j, k, n, p, q)\n",
    "    \n",
    "#     # Redefine free variable characteristic as category and reorder the categories as follows: Flow Pressure, Magnitude, Bz, Speed, Kp, AE, Dst\n",
    "#     # Must be done to capture coefficients correponding to each variable in the correct order\n",
    "#     loc['free_var'] = loc.free_var.astype('category')\n",
    "#     loc.free_var.cat.set_categories(\n",
    "#         new_categories=['SW Flow Pressure (nPa)','SW Bmag (nT)','SW Bz (nT)','SW Velocity (km/sec)','Kp (n/a)','AE (hourly) (nT)','Dst (nT)'],ordered=True\n",
    "#     )\n",
    "    \n",
    "#     # Get intervals of the configuration as tuples (pairs)\n",
    "#     p_interval = list(pairwise(boundaries[0]))[i]\n",
    "#     bmag_interval = list(pairwise(boundaries[1]))[j]\n",
    "#     bz_interval = list(pairwise(boundaries[2]))[k]\n",
    "#     v_interval = list(pairwise(boundaries[3]))[n]\n",
    "#     kp_interval = list(pairwise(boundaries[4]))[p]\n",
    "#     ae_interval = list(pairwise(boundaries[5]))[q]\n",
    "    \n",
    "#     # Compiled coefficients with order as follows: Flow Pressure coefficient, Magnitude coefficient, Bz coefficient, Speed coefficient, Kp coefficient, AE coefficient\n",
    "#     # Reindex coefficients in the specified order with the set size as 6, and replace any missing coefficients with 0\n",
    "#     coefficients = loc[['free_var','coefficient']].set_index(['free_var']).reindex(np.arange(6),fill_value=0)['coefficient'].to_list()\n",
    "#     #coefficients = loc.coefficient.tolist()\n",
    "    \n",
    "#     # All data with the matching configuration\n",
    "#     filtered = result[(result['SW Flow Pressure (nPa)']>p_interval[0]) & (result['SW Flow Pressure (nPa)']<=p_interval[1]) & \n",
    "#                 (result['SW Bmag (nT)']>bmag_interval[0]) & (result['SW Bmag (nT)']<=bmag_interval[1]) &\n",
    "#                 (result['SW Bz (nT)']>bz_interval[0]) & (result['SW Bz (nT)']<=bz_interval[1]) &\n",
    "#                 (result['SW Velocity (km/sec)']>v_interval[0]) & (result['SW Velocity (km/sec)']<=v_interval[1]) &\n",
    "#                 (result['Kp (n/a)']>kp_interval[0]) & (result['Kp (n/a)']<=kp_interval[1]) &\n",
    "#                 (result['AE (hourly) (nT)']>ae_interval[0]) & (result['AE (hourly) (nT)']<=ae_interval[1]) &\n",
    "#                 (result['Dst (nT)']>dst_interval[0]) & (result['Dst (nT)']<=dst_interval[1])]\n",
    "    \n",
    "#     # Optimize/determine intercept for general function using the data with the matching config\n",
    "#     min_scalar = minimize_scalar(weight_func, args=(filtered, coefficients))\n",
    "#     intercept = min_scalar.x\n",
    "    \n",
    "#     # Calculate estimate for Dst by using the compiled coefficients and intercept\n",
    "#     filtered['Dst est'] = (filtered['SW Flow Pressure (nPa)'] * coefficients[0] \\\n",
    "#             + filtered['SW Bmag (nT)'] * coefficients[1] \\\n",
    "#             + filtered['SW Bz (nT)'] * coefficients[2] \\\n",
    "#             + filtered['SW Velocity (km/sec)'] * coefficients[3] \\\n",
    "#             + filtered['Kp (n/a)'] * coefficients[4] \\\n",
    "#             + filtered['AE (hourly) (nT)'] * coefficients[5] \\\n",
    "#             + intercept)\n",
    "    \n",
    "#     result.update(filtered, overwrite=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
